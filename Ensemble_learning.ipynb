{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e934a9cd",
   "metadata": {},
   "source": [
    "# 集成学习(Ensemble Learning)\n",
    "**姓名:** 独宇涵 **邮箱:** 231880151@smail.nju.edu.cn\n",
    "\n",
    "**实验环境:** python 3.12.8\n",
    "\n",
    "## 复现材料\n",
    "实验过程中需要我们手动实现三种常见的集成学习算法（**Stacking**、**Bagging**、**AdaBoost**），下面是三种算法的代码实现\n",
    "### Stacking\n",
    "**输入:** 训练集 $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)\\};$\n",
    "\n",
    "初级学习算法 $\\mathfrak{L}_1, \\mathfrak{L}_2, \\ldots, \\mathfrak{L}_T;$\n",
    "\n",
    "次级学习算法 $\\mathfrak{L}.$\n",
    "\n",
    "**过程：**\n",
    "1. $\\text{for } t = 1, 2, \\ldots, T \\text{ do}$\n",
    "2. $h_t = \\mathfrak{L}_t(D);$\n",
    "3. $\\text{end for}$\n",
    "4. $D' = \\emptyset;$\n",
    "5. $\\text{for } i = 1, 2, \\ldots, m \\text{ do}$\n",
    "6. $\\quad \\text{for } t = 1, 2, \\ldots, T \\text{ do}$\n",
    "7. $\\quad z_{it} = h_t(x_i);$\n",
    "8. $\\quad \\text{end for}$\n",
    "9. $D' = D' \\cup ((z_{i1}, z_{i2}, \\ldots, z_{iT}), y_i);$\n",
    "10. $\\text{end for}$\n",
    "11. $h' = \\mathfrak{L}(D');$\n",
    "\n",
    "**输出：**$H(x) = h'(h_1(x), h_2(x), \\ldots, h_T(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c35e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(2025)\n",
    "def get_stacking(clf,x_train,y_train,x_test,n_folds = 10):\n",
    "    # 核心是使用交叉验证算法得到次级训练集\n",
    "    train_num, test_num = x_train.shape[0], x_test.shape[0]\n",
    "    second_level_train_set = np.zeros((train_num,))\n",
    "    second_level_test_set = np.zeros((test_num,))\n",
    "    test_nfolds_sets = np.zeros((test_num,n_folds))\n",
    "    kf = KFold(n_splits = n_folds) #将数据进行折叠\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "        x_tst, y_tst = x_train[test_index], y_train[test_index]\n",
    "\n",
    "        clf.fit(x_tra, y_tra)\n",
    "\n",
    "        second_level_train_set[test_index] = clf.predict(x_tst)\n",
    "        test_nfolds_sets[:,i] = clf.predict(x_test)\n",
    "\n",
    "    second_level_test_set[:] = test_nfolds_sets.mean(axis = 1)\n",
    "    return second_level_train_set, second_level_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89171706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy:0.9333\n"
     ]
    }
   ],
   "source": [
    "#构造初级和次级学习器\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier,BaggingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "adb_model = AdaBoostClassifier()\n",
    "gdbc_model = GradientBoostingClassifier()\n",
    "et_model = ExtraTreesClassifier()\n",
    "svc_model = SVC()\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "train_x,test_x,train_y,test_y = train_test_split(iris.data, iris.target, test_size = 0.2)\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "base_classifiers = [rf_model, adb_model, gdbc_model, et_model, svc_model]\n",
    "\n",
    "for clf in base_classifiers:\n",
    "    train_set, test_set = get_stacking(clf, train_x, train_y, test_x)\n",
    "    train_sets.append(train_set)\n",
    "    test_sets.append(test_set)\n",
    "\n",
    "meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis = 1)\n",
    "meta_test = np.concatenate([result_set.reshape(-1,1) for result_set in test_sets], axis = 1)\n",
    "\n",
    "# 使用决策树作为次级分类器\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "meta_clf = DecisionTreeClassifier()\n",
    "meta_clf.fit(meta_train,train_y)\n",
    "predict_y = meta_clf.predict(meta_test)\n",
    "\n",
    "accuracy = accuracy_score(test_y, predict_y)\n",
    "print(f\"Stacking Classifier Accuracy:{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4700d8",
   "metadata": {},
   "source": [
    "#### 封装为自定义类\n",
    "上面实现了简单的一个运用Stacking集成学习方法的分类器，现在将上述算法封装为自定义的类，方便后续调用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ce5afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy:0.9333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin,clone\n",
    "\n",
    "class StackingAverageModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models,meta_model, n_folds = 5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def fit(self, X, y): #克隆原来的model，并且实现fit功能\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits = self.n_folds, shuffle = True, random_state = 156)\n",
    "\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X,y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "            \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([np.column_stack([model.predict(X) for model in base_models]).mean(axis = 1) for base_models in self.base_models_])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "        \n",
    "stacking_avg = StackingAverageModels(base_models = base_classifiers, meta_model = meta_clf)\n",
    "stacking_avg.fit(train_x, train_y)\n",
    "pred_y = stacking_avg.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(f\"Stacking Classifier Accuracy:{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32036f9",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "**输入：**\n",
    "\n",
    "训练集 $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)\\}$;\n",
    "\n",
    "基学习算法 $\\mathcal{L}$;\n",
    "    \n",
    "训练轮数 $T$.\n",
    "\n",
    "**过程：**\n",
    "\n",
    "1: for $t = 1, 2, \\ldots, T$ do\n",
    "\n",
    "2: $\\quad h_t = \\mathcal{L}(D, \\mathcal{D}_{bs})$\n",
    "\n",
    "3: end for\n",
    "\n",
    "输出：$H(x) = \\arg\\max_{y \\in \\mathcal{Y}} \\sum_{t=1}^{T} \\mathbb{I}(h_t(x) = y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a803ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBaggingClassifier:\n",
    "    def __init__(self, base_learner, n_learners):\n",
    "        self.learners = [clone(base_learner) for _ in range(n_learners)]\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        for learner in self.learners:\n",
    "            examples = np.random.choice(np.arange(len(X)),int(len(X)),replace = True)\n",
    "            learner.fit(X[examples,:],y[examples])\n",
    "\n",
    "    def predict(self,X):\n",
    "        preds = [learner.predict(X) for learner in self.learners]\n",
    "        return self._aggregate(np.array(preds))\n",
    "    \n",
    "    def _aggregate(self, predictions):\n",
    "        final_pred = np.apply_along_axis(lambda x:np.bincount(x).argmax(),axis = 0, arr = predictions.astype(int))\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178cf5e",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "**输入：**\n",
    "    训练集 $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)\\}$；\n",
    "    基学习算法 $\\mathcal{L}$；\n",
    "    训练轮数 $T$。\n",
    "\n",
    "**过程：**\n",
    "1. $\\mathcal{D}_1(x) = 1/m$。\n",
    "2. for $t = 1, 2, \\ldots, T$ do\n",
    "3. $\\quad h_t = \\mathcal{L}(D, \\mathcal{D}_t)$;\n",
    "4. $\\quad \\epsilon_t = P_{x \\sim \\mathcal{D}_t}(h_t(x) \\neq f(x))$;\n",
    "5. $\\quad if \\epsilon_t > 0.5$ then break\n",
    "6. $\\quad \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)$;\n",
    "7. $\\quad \\mathcal{D}_{t+1}(x) = \\frac{\\mathcal{D}_t(x)}{Z_t} \\times \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\exp(-\\alpha_t), & \\text{if } h_t(x) = f(x) \\\\\n",
    "\\exp(\\alpha_t), & \\text{if } h_t(x) \\neq f(x)\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "$\\quad \\quad \\quad \\quad \\quad \\quad \\quad  = \\mathcal{D}_t(x) \\exp(-\\alpha_t f(x) h_t(x)) / Z_t$\n",
    "\n",
    "8. end for\n",
    "\n",
    "**输出：**$H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0aea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdaBoostClassifier:\n",
    "    def __init__(self, base_learner, n_learners):\n",
    "        self.learners = [clone(base_learner) for _ in range(n_learners)]\n",
    "        self.learning_rate = 1.0\n",
    "        self.weak_classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        sample_weights = np.ones_like(y) / len(y)\n",
    "        for clf in self.learners:\n",
    "            clf = DecisionTreeClassifier(max_depth = 1)\n",
    "            clf.fit(X, y, sample_weight=sample_weights)\n",
    "            y_pred = clf.predict(X)\n",
    "            incorrect = np.sum(sample_weights * (y != y_pred))\n",
    "            error_rate = incorrect / np.sum(sample_weights)\n",
    "            if error_rate > 0.5:\n",
    "                continue\n",
    "\n",
    "            alpha = np.log((1.0 - error_rate) / error_rate) / 2.0\n",
    "            self.weak_classifiers.append((clf, alpha))\n",
    "            sample_weights *= np.exp(-alpha * y * y_pred)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0],))\n",
    "        for clf, alpha in self.weak_classifiers:\n",
    "            votes += alpha * clf.predict(X)\n",
    "        return np.sign(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17dc3c",
   "metadata": {},
   "source": [
    "### 与sklearn的实现进行对比\n",
    "将自主实现的三种集成学习算法与sklearn库中已有的方法进行性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "270835db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Stacking Classifier Accuracy: 0.9333\n",
      "Sklearn Stacking Classifier Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "stacking_avg = StackingAverageModels(base_models=base_classifiers, meta_model=meta_clf)\n",
    "stacking_avg.fit(train_x, train_y)\n",
    "pred_y = stacking_avg.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(f\"My Stacking Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# base_classifier的格式与上面我们自己实现的稍有不同\n",
    "base_classifiers = [\n",
    "    ('rf', rf_model),\n",
    "    ('adb', adb_model),\n",
    "    ('gdbc', gdbc_model),\n",
    "    ('et', et_model),\n",
    "    ('svc', svc_model)\n",
    "]\n",
    "stacking_clf = StackingClassifier(estimators=base_classifiers,final_estimator=meta_clf, cv=5)\n",
    "stacking_clf.fit(train_x, train_y)\n",
    "pred_y = stacking_clf.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(f\"Sklearn Stacking Classifier Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c40334da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Bagging Classifier Accuracy: 0.9333\n",
      "Sklearn Bagging Classifier Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "base_classifier = DecisionTreeClassifier()\n",
    "my_bagging_clf = MyBaggingClassifier(base_learner=base_classifier,n_learners=100)\n",
    "my_bagging_clf.fit(train_x, train_y)\n",
    "pred_y = my_bagging_clf.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(f\"My Bagging Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_classifier,n_estimators=100,max_samples=0.5,max_features=0.5,random_state=42)\n",
    "bagging_clf.fit(train_x, train_y)\n",
    "pred_y = bagging_clf.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(f\"Sklearn Bagging Classifier Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6384797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My AdaBoost Classifier Accuracy: 0.8800\n",
      "Sklearn AdaBoost Classifier Accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,n_redundant=0, random_state=42)\n",
    "y = np.where(y == 0,-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "dt_clf = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "my_adaboost_clf = MyAdaBoostClassifier(dt_clf, n_learners=50)\n",
    "my_adaboost_clf.fit(X_train, y_train)\n",
    "pred_y = my_adaboost_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred_y)\n",
    "print(f\"My AdaBoost Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "pred_y = adaboost_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred_y)\n",
    "print(f\"Sklearn AdaBoost Classifier Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
