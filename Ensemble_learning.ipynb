{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e934a9cd",
   "metadata": {},
   "source": [
    "# 集成学习(Ensemble Learning)\n",
    "**姓名:** 独宇涵 **邮箱:** 231880151@smail.nju.edu.cn\n",
    "\n",
    "**实验环境:** python 3.12.8\n",
    "\n",
    "## 复现材料\n",
    "实验过程中需要我们手动实现三种常见的集成学习算法（**Stacking**、**Bagging**、**AdaBoost**），下面是三种算法的代码实现\n",
    "### Stacking\n",
    "**输入:** 训练集 $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)\\};$\n",
    "\n",
    "初级学习算法 $\\mathfrak{L}_1, \\mathfrak{L}_2, \\ldots, \\mathfrak{L}_T;$\n",
    "\n",
    "次级学习算法 $\\mathfrak{L}.$\n",
    "\n",
    "**过程：**\n",
    "1. $\\text{for } t = 1, 2, \\ldots, T \\text{ do}$\n",
    "2. $h_t = \\mathfrak{L}_t(D);$\n",
    "3. $\\text{end for}$\n",
    "4. $D' = \\emptyset;$\n",
    "5. $\\text{for } i = 1, 2, \\ldots, m \\text{ do}$\n",
    "6. $\\quad \\text{for } t = 1, 2, \\ldots, T \\text{ do}$\n",
    "7. $\\quad z_{it} = h_t(x_i);$\n",
    "8. $\\quad \\text{end for}$\n",
    "9. $D' = D' \\cup ((z_{i1}, z_{i2}, \\ldots, z_{iT}), y_i);$\n",
    "10. $\\text{end for}$\n",
    "11. $h' = \\mathfrak{L}(D');$\n",
    "\n",
    "**输出：**$H(x) = h'(h_1(x), h_2(x), \\ldots, h_T(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c35e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(2025)\n",
    "def get_stacking(clf,x_train,y_train,x_test,n_folds = 10):\n",
    "    # 核心是使用交叉验证算法得到次级训练集\n",
    "    train_num, test_num = x_train.shape[0], x_test.shape[0]\n",
    "    second_level_train_set = np.zeros((train_num,))\n",
    "    second_level_test_set = np.zeros((test_num,))\n",
    "    test_nfolds_sets = np.zeros((test_num,n_folds))\n",
    "    kf = KFold(n_splits = n_folds) #将数据进行折叠\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "        x_tst, y_tst = x_train[test_index], y_train[test_index]\n",
    "\n",
    "        clf.fit(x_tra, y_tra)\n",
    "\n",
    "        second_level_train_set[test_index] = clf.predict(x_tst)\n",
    "        test_nfolds_sets[:,i] = clf.predict(x_test)\n",
    "\n",
    "    second_level_test_set[:] = test_nfolds_sets.mean(axis = 1)\n",
    "    return second_level_train_set, second_level_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89171706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy:0.9333\n"
     ]
    }
   ],
   "source": [
    "#构造初级和次级学习器\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "adb_model = AdaBoostClassifier()\n",
    "gdbc_model = GradientBoostingClassifier()\n",
    "et_model = ExtraTreesClassifier()\n",
    "svc_model = SVC()\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "train_x,test_x,train_y,test_y = train_test_split(iris.data, iris.target, test_size = 0.2)\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "base_classifiers = [rf_model, adb_model, gdbc_model, et_model, svc_model]\n",
    "\n",
    "for clf in base_classifiers:\n",
    "    train_set, test_set = get_stacking(clf, train_x, train_y, test_x)\n",
    "    train_sets.append(train_set)\n",
    "    test_sets.append(test_set)\n",
    "\n",
    "meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis = 1)\n",
    "meta_test = np.concatenate([result_set.reshape(-1,1) for result_set in test_sets], axis = 1)\n",
    "\n",
    "# 使用决策树作为次级分类器\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "meta_clf = DecisionTreeClassifier()\n",
    "meta_clf.fit(meta_train,train_y)\n",
    "predict_y = meta_clf.predict(meta_test)\n",
    "\n",
    "accuracy = accuracy_score(test_y, predict_y)\n",
    "print(f\"Stacking Classifier Accuracy:{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4700d8",
   "metadata": {},
   "source": [
    "#### 封装为自定义类\n",
    "上面实现了简单的一个运用Stacking集成学习方法的分类器，现在将上述算法封装为自定义的类，方便后续调用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ce5afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 25 features, but DecisionTreeClassifier is expecting 5 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m stacking_avg = StackingAverageModels(base_models = base_classifiers, meta_model = meta_clf)\n\u001b[32m     32\u001b[39m stacking_avg.fit(train_x, train_y)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m pred_y = \u001b[43mstacking_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m accuracy = accuracy_score(test_y, pred_y)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStacking Classifier Accuracy:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mStackingAverageModels.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m     28\u001b[39m     meta_features = np.column_stack([np.column_stack([model.predict(X) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m base_models]) \u001b[38;5;28;01mfor\u001b[39;00m base_models \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_models_])\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeta_model_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lab/lib/python3.12/site-packages/sklearn/tree/_classes.py:530\u001b[39m, in \u001b[36mBaseDecisionTree.predict\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[32m    508\u001b[39m \n\u001b[32m    509\u001b[39m \u001b[33;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    527\u001b[39m \u001b[33;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m proba = \u001b[38;5;28mself\u001b[39m.tree_.predict(X)\n\u001b[32m    532\u001b[39m n_samples = X.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lab/lib/python3.12/site-packages/sklearn/tree/_classes.py:489\u001b[39m, in \u001b[36mBaseDecisionTree._validate_X_predict\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    488\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    498\u001b[39m     X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc\n\u001b[32m    499\u001b[39m ):\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lab/lib/python3.12/site-packages/sklearn/utils/validation.py:2965\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/lab/lib/python3.12/site-packages/sklearn/utils/validation.py:2829\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2830\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2831\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2832\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 25 features, but DecisionTreeClassifier is expecting 5 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin,clone\n",
    "\n",
    "class StackingAverageModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models,meta_model, n_folds = 5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def fit(self, X, y): #克隆原来的model，并且实现fit功能\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits = self.n_folds, shuffle = True, random_state = 156)\n",
    "\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X,y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "            \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([np.column_stack([model.predict(X) for model in base_models]) for base_models in self.base_models_])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "        \n",
    "stacking_avg = StackingAverageModels(base_models = base_classifiers, meta_model = meta_clf)\n",
    "stacking_avg.fit(train_x, train_y)\n",
    "pred_y = stacking_avg.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, pred_y)\n",
    "print(f\"Stacking Classifier Accuracy:{accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
